#

## üß† **ConversationContext Klasse - Hauptfunktionen**

### **Kontextverwaltung**

- `set(key, value)` - Setzt Kontextwerte
- `get(key)` - Holt Kontextwerte
- `has(key)` - Pr√ºft Kontextexistenz
- `delete(key)` - L√∂scht Kontext
- `clear()` - Setzt gesamten Kontext zur√ºck
- `resetContext(keepHistory)` - Reset mit History-Option

### **Analyse-Funktionen**

- `update(messages, responseTime)` - Aktualisiert Kontext basierend auf Nachrichten
- `matchRules(input)` - Regelbasiertes Matching f√ºr Eingaben
- `executeAction(action, params)` - F√ºhrt Aktionen/Tools/Workflows aus

### **Datenabfrage**

- `getContext()` - Gibt gesamten Kontextzustand zur√ºck
- `getPreferences()` - Gibt Benutzerpr√§ferenzen zur√ºck
- `getDiagnostics()` - Systemdiagnose-Informationen
- `mergeInto(target)` - Merged Kontext in Zielobjekt

## üîç **Analysierte Themenkategorien**

Die KI erkennt folgende Themenbereiche:

**ERP-Bereiche:**

- `orders` (Bestellungen/Auftr√§ge)
- `inventory` (Lager/Bestand)
- `customers` (Kunden)
- `invoices` (Rechnungen/Zahlungen)
- `finance` (Finanzen/Umsatz)

**Technische Bereiche:**

- `database` (Datenbank/SQL)
- `file_operations` (Dateioperationen)
- `ai` (KI/Modelle/Workflows)
- `system_monitoring` (System√ºberwachung)
- `code` (Programmierung)

**Kommunikation:**

- `greetings` (Begr√º√üungen)
- `thanks` (Dank)
- `goodbye` (Verabschiedungen)
- `communication` (Chat/Kommunikation)

## ‚öôÔ∏è **Integrierte Aktionen**

### **Tool-Integration**

- Zugriff auf `toolRegistry` f√ºr Tool-Execution
- Automatisches Tracking von Tool-Nutzung

### **Workflow-Integration**

- Integration mit `workflowEngine`
- Workflow-Execution und Monitoring

## üìä **Statistik-Tracking**

- Nachrichtenanzahl und Response-Zeiten
- Themenwechsel
- Ausgel√∂ste Regeln
- Genutzte Tools/Workflows
- Konfidenz-Bewertung

## üéØ **Intent-Erkennung**

- `query` (Abfragen/Anzeigen)
- `create` (Erstellen/Hinzuf√ºgen)
- `update` (Aktualisieren/√Ñndern)
- `delete` (L√∂schen/Entfernen)
- `calculate` (Berechnen/Simulieren)
- `diagnose` (Testen/Diagnostizieren)
- `explain` (Erkl√§ren/Beschreiben)

Die Komponente dient als intelligente Kontextverwaltung f√ºr ERP-Dialoge mit erweiterter Themenanalyse, Regelverarbeitung und Tool-Integration.

Basierend auf der analysierten `anthropicProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Anthropic Provider - Hauptfunktionen**

### **Kernfunktionen**

- `callAnthropic(model, messages, options, context)` - Hauptfunktion f√ºr KI-Aufrufe
- `initializeAnthropicClient()` - Client-Initialisierung
- `getAnthropicClient()` - Client-Abruf

### **Message Processing**

- `mapMessages(messages)` - Mappt Chat-Nachrichten f√ºr Anthropic API
- `prepareToolsForAnthropic()` - Bereitet Tools f√ºr API-Aufruf vor

### **Tool Execution System**

- `detectAndRunTools(output, config)` - Erkennt und f√ºhrt Tools aus
- `parseToolParams(paramString, pattern)` - Parameter-Parsing
- `executeToolCall(toolName, params)` - F√ºhrt Tool-Aufrufe aus
- `validateToolParameters(tool, params)` - Parameter-Validierung

### **Utility Functions**

- `isAnthropicModel(modelId)` - Pr√ºft auf Anthropic-Modelle
- `getSupportedAnthropicModels()` - Liste unterst√ºtzter Modelle
- `validateAnthropicConfig()` - Konfigurationsvalidierung
- `createFallbackResponse()` - Fallback bei Fehlern
- `formatToolResults(results)` - Formatierung von Tool-Ergebnissen

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export const anthropicProvider = {
  name: 'anthropic',
  call: callAnthropic,                    // Hauptaufruffunktion
  isSupportedModel: isAnthropicModel,     // Modellpr√ºfung
  getSupportedModels: getSupportedAnthropicModels, // Modellliste
  validateConfig: validateAnthropicConfig, // Konfigurationscheck
  healthCheck(): Promise<{healthy, details}> // Gesundheitspr√ºfung
}
```

## üõ†Ô∏è **Tool Call Patterns**

Der Provider erkennt mehrere Tool-Aufruf-Formate:

### **Erkannte Syntaxformen**

1. **Direct Tool Call**: `#TOOL: tool_name(params)`
2. **Code Block Format**: `tool tool_name params`
3. **JSON Format**: `{"tool": "tool_name", "params": {...}}`

### **Parameter-Parsing**

- JSON-Parsing f√ºr komplexe Parameter
- Key=Value Parsing f√ºr einfache Syntax
- Automatische Typkonvertierung (Boolean, Number, String)

## üìä **Response-Verarbeitung**

### **Response-Struktur**

```typescript
ModelResponse {
  model: string,
  provider: 'anthropic',
  text: string,
  tokens_in: number,
  tokens_out: number,
  duration_ms: number,
  tool_calls: Array,
  success: boolean,
  meta: {
    stop_reason: string,
    tool_results: ToolResult[],
    response_time: number
  }
}
```

### **Tool Results Integration**

- Automatisches Anh√§ngen von Tool-Ergebnissen an Antwort
- Erfolgs-/Fehler-Zusammenfassung
- Laufzeit-Metriken

## üîê **Konfiguration & Validierung**

### **Umgebungsvariablen**

- `ANTHROPIC_API_KEY` - Erforderlicher API-Schl√ºssel

### **Provider-Konfiguration**

```typescript
AnthropicProviderConfig {
  maxTokens?: number,           // Maximale Tokens
  temperature?: number,         // Kreativit√§t
  timeoutMs?: number,           // Timeout in ms
  enableToolCalls?: boolean,    // Tool-Unterst√ºtzung
  enableWorkflows?: boolean,    // Workflow-Unterst√ºtzung
  toolCallPatterns?: RegExp[],  // Custom Tool-Patterns
  fallbackOnError?: boolean,    // Fallback bei Fehlern
  debugMode?: boolean          // Debug-Informationen
}
```

## üìã **Unterst√ºtzte Modelle**

### **Claude 3.5 Serie**

- `claude-3-5-sonnet-20241022`

### **Claude 3 Serie**

- `claude-3-opus-20240229`
- `claude-3-sonnet-20240229`
- `claude-3-haiku-20240307`

### **Claude 2 Serie**

- `claude-2.1`
- `claude-2.0`
- `claude-instant-1.2`

## üö® **Fehlerbehandlung**

### **Fallback-System**

- Automatische Fallback-Antworten bei API-Fehlern
- Detaillierte Fehlerprotokollierung
- Timeout-Management (30s Standard)

### **Health Check**

- Konfigurationsvalidierung
- Test-API-Aufruf
- Detaillierte Statusinformationen

## üîÑ **Integrationen**

### **Tool Registry**

- Integration mit `toolRegistry.getToolDefinitions()`
- Automatische Tool-Validierung
- Parameter-Schema-√úberpr√ºfung

### **Conversation Context**

- Kontextaktualisierung nach Antworten
- Response-Time Tracking
- Themenanalyse-Integration

Der Provider dient als vollst√§ndige Integration der Anthropic Claude API mit erweiterter Tool-Unterst√ºtzung, Fehlerbehandlung und Kontextintegration f√ºr das ERP-System.

Basierend auf der analysierten `azureOpenAIProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Azure OpenAI Provider - Hauptfunktionen**

### **Kernfunktionen**

- `callAzureOpenAI(model, messages, options, context)` - Hauptfunktion f√ºr Azure OpenAI Aufrufe
- `initializeAzureClient()` - Client-Initialisierung
- `getAzureClient()` - Client-Abruf
- `getAzureClientConfig()` - Konfigurationsabruf

### **Message Processing**

- `prepareOpenAIMessages(messages, config)` - Bereitet Nachrichten f√ºr API vor
- `prepareToolsForOpenAI()` - Bereitet Tools f√ºr OpenAI Format vor

### **Tool Execution System**

- `executeToolCalls(toolCalls)` - F√ºhrt Tool-Aufrufe aus (OpenAI Format)
- `formatToolResults(results)` - Formatierung von Tool-Ergebnissen

### **Utility Functions**

- `isAzureOpenAIModel(modelId)` - Pr√ºft auf Azure OpenAI-Modelle
- `getSupportedAzureModels()` - Liste unterst√ºtzter Modelle
- `validateAzureConfig()` - Konfigurationsvalidierung
- `createFallbackResponse()` - Fallback bei Fehlern

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export const azureOpenAIProvider = {
  name: 'azure',
  call: callAzureOpenAI,                    // Hauptaufruffunktion
  isSupportedModel: isAzureOpenAIModel,     // Modellpr√ºfung
  getSupportedModels: getSupportedAzureModels, // Modellliste
  validateConfig: validateAzureConfig,      // Konfigurationscheck
  config: azureConfig,                      // Provider-Konfiguration
  healthCheck(): Promise<{healthy, details}>, // Gesundheitspr√ºfung
  resetClient(): void                       // Client-Reset
}
```

## ‚öôÔ∏è **Konfigurationssystem**

### **Umgebungsvariablen**

- `AZURE_OPENAI_API_KEY` - API-Schl√ºssel (erforderlich)
- `AZURE_OPENAI_ENDPOINT` - Endpoint URL (erforderlich)
- `AZURE_OPENAI_DEPLOYMENT` - Deployment-Name (erforderlich)
- `AZURE_OPENAI_API_VERSION` - API Version (optional)
- `AZURE_OPENAI_TEMPERATURE` - Temperatureinstellung
- `AZURE_OPENAI_MAX_TOKENS` - Maximale Tokens

### **Provider-Konfiguration**

```typescript
AzureOpenAIProviderConfig {
  maxTokens?: number,           // Maximale Tokens (Standard: 1500)
  temperature?: number,         // Kreativit√§t (Standard: 0.4)
  timeoutMs?: number,           // Timeout in ms (Standard: 30000)
  enableToolCalls?: boolean,    // Tool-Unterst√ºtzung (Standard: true)
  enableStreaming?: boolean,    // Streaming (Standard: false)
  fallbackOnError?: boolean,    // Fallback bei Fehlern (Standard: true)
  apiVersion?: string,          // API Version
  deploymentName?: string       // Deployment Name
}
```

## üõ†Ô∏è **Tool Integration**

### **OpenAI Tool Calling Format**

- Native Integration mit OpenAI Function Calling
- Automatische Tool-Definition aus Registry
- Parameter-Schema-Validierung

### **Tool Execution Flow**

1. API erkennt Tool-Aufrufe in Response
2. `executeToolCalls()` verarbeitet Tool-Calls
3. Parameter werden als JSON geparst
4. Tools werden √ºber Registry ausgef√ºhrt
5. Ergebnisse werden formatiert und angeh√§ngt

## üìä **Response-Verarbeitung**

### **Response-Struktur**

```typescript
ModelResponse {
  model: string,
  provider: 'azure',
  text: string,
  tokens_in: number,
  tokens_out: number,
  duration_ms: number,
  tool_calls: any[],
  success: boolean,
  meta: {
    finish_reason: string,
    tool_results: ToolResult[],
    deployment: string,
    api_version: string
  }
}
```

## üìã **Unterst√ºtzte Modelle**

### **GPT-4 Serie**

- `gpt-4`
- `gpt-4-32k`
- `gpt-4-turbo`
- `gpt-4o`

### **GPT-3.5 Serie**

- `gpt-35-turbo`
- `gpt-35-turbo-16k`
- `gpt-35-turbo-instruct`

### **Azure Deployment-Namen**

- Unterst√ºtzung f√ºr benutzerdefinierte Deployment-Namen
- Automatische Erkennung von Azure-Modellen

## üîê **Sicherheit & Validierung**

### **Konfigurationsvalidierung**

- API-Key Validierung
- Endpoint-URL Validierung
- Deployment-Name Pr√ºfung
- URL-Format Validierung

### **Health Check System**

- Konfigurationspr√ºfung
- Test-API-Aufruf
- Detaillierte Fehlerberichte

## üö® **Fehlerbehandlung**

### **Fallback-System**

- Automatische Fallback-Antworten bei API-Fehlern
- Timeout-Management (30s Standard)
- Detaillierte Fehlerprotokollierung

### **Client Management**

- Client-Caching f√ºr Performance
- Reset-Funktion f√ºr Re-Initialisierung
- Parallele Initialisierungsverhinderung

## üîÑ **Integrationen**

### **Tool Registry**

- Integration mit `toolRegistry.getToolDefinitions()`
- Automatische Tool-Validierung
- Parameter-Schema-√úberpr√ºfung

### **Conversation Context**

- Kontextaktualisierung nach Antworten
- Response-Time Tracking
- Themenanalyse-Integration

## üåê **API-Kompatibilit√§t**

### **Azure OpenAI Service**

- Kompatibel mit Azure OpenAI Deployment
- Unterst√ºtzt verschiedene API-Versionen
- Deployment-basierte Authentifizierung

### **OpenAI SDK**

- Verwendet offizielle OpenAI SDK
- Unterst√ºtzt Function Calling
- Kompatibel mit Chat-Completions API

Der Provider bietet eine vollst√§ndige Integration des Azure OpenAI Services mit erweiterter Tool-Unterst√ºtzung, robustem Fehlerhandling und Enterprise-f√§higer Konfiguration f√ºr das ERP-System.

Basierend auf der analysierten `customProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Custom Provider - Hauptfunktionen**

### **Kernfunktionen**

- `callCustomAPI(model, messages, options, context)` - Hauptfunktion f√ºr Custom API Aufrufe
- `buildHeaders()` - Erstellt Request-Header
- `buildRequestPayload()` - Baut Request-Payload auf

### **Message Processing**

- `prepareMessages(messages, format)` - Bereitet Nachrichten f√ºr API vor
- `prepareToolsForCustomAPI()` - Bereitet Tools f√ºr Custom API vor

### **Response Processing**

- `processCustomResponse(data, model, duration, config)` - Verarbeitet API-Response
- `extractField(data, paths)` - Extrahiert Felder aus Response
- `extractToolCalls(data)` - Erkennt Tool-Aufrufe in Response

### **Tool Execution**

- `executeToolCalls(toolCalls)` - F√ºhrt Tool-Aufrufe aus
- `formatToolResults(results)` - Formatierung von Tool-Ergebnissen

### **Utility Functions**

- `isCustomModel(modelId)` - Pr√ºft auf Custom-Modelle
- `testCustomAPI()` - Testet API-Verbindung
- `validateCustomConfig()` - Konfigurationsvalidierung
- `createFallbackResponse()` - Fallback bei Fehlern

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export const customProvider = {
  name: 'custom',
  call: callCustomAPI,              // Hauptaufruffunktion
  isSupportedModel: isCustomModel,   // Modellpr√ºfung
  testConnection: testCustomAPI,     // Verbindungstest
  validateConfig: validateCustomConfig, // Konfigurationscheck
  config: customConfig,              // Provider-Konfiguration
  healthCheck(): Promise<{healthy, details}>, // Gesundheitspr√ºfung
  updateConfig(newConfig): void     // Dynamische Konfiguration
}
```

## ‚öôÔ∏è **Flexibles Konfigurationssystem**

### **Umgebungsvariablen**

- `CUSTOM_AI_URL` - API Endpoint URL (erforderlich)
- `CUSTOM_AI_KEY` - API-Schl√ºssel (optional)
- `CUSTOM_AI_MODEL` - Modellname (Standard: "generic")
- `CUSTOM_AI_TEMPERATURE` - Temperatureinstellung
- `CUSTOM_AI_MAX_TOKENS` - Maximale Tokens
- `CUSTOM_AI_FORMAT` - Request-Format
- `CUSTOM_AI_AUTH_TYPE` - Authentifizierungstyp
- `CUSTOM_AI_HEADERS` - Benutzerdefinierte Header (JSON)
- `CUSTOM_AI_PARAMS` - Benutzerdefinierte Parameter (JSON)

### **Provider-Konfiguration**

```typescript
CustomProviderConfig {
  timeoutMs?: number,           // Timeout in ms (Standard: 60000)
  retryAttempts?: number,       // Retry-Versuche (Standard: 2)
  enableToolCalls?: boolean,    // Tool-Unterst√ºtzung (Standard: true)
  fallbackOnError?: boolean,    // Fallback bei Fehlern (Standard: true)
  requestFormat?: string,       // 'openai' | 'anthropic' | 'generic' | 'custom'
  responseMapping?: {           // Response-Feld-Mapping
    text?: string[],           // M√∂gliche Text-Felder
    error?: string[],          // M√∂gliche Error-Felder
    tokens?: string[]          // M√∂gliche Token-Felder
  }
}
```

## üåê **Unterst√ºtzte API-Formate**

### **Request-Formate**

1. **OpenAI Format** - Kompatibel mit OpenAI API
2. **Anthropic Format** - Kompatibel mit Claude API
3. **Generic Format** - Einfaches Chat-Format
4. **Custom Format** - Vollst√§ndig anpassbar

### **Authentifizierungstypen**

- `bearer` - Bearer Token Authentication
- `api_key` - API Key Header
- `token` - Custom Token Header
- Custom - Vollst√§ndig anpassbar

## üõ†Ô∏è **Tool Integration**

### **Flexible Tool-Call Erkennung**

- Unterst√ºtzt verschiedene Tool-Call Formate:
  - `tool_calls` Array
  - `tools` Array
  - `function_calls` Array
- Automatische Parameter-Extraktion

### **Tool Execution**

- Integration mit `toolRegistry`
- Parameter-Validierung
- Fehlerbehandlung

## üìä **Response-Verarbeitung**

### **Intelligentes Field-Mapping**

```typescript
responseMapping: {
  text: ['text', 'response', 'message', 'content', 'answer'],
  error: ['error', 'error_message', 'err'],
  tokens: ['tokens', 'usage.total_tokens', 'usage.tokens']
}
```

### **Response-Struktur**

```typescript
ModelResponse {
  model: string,
  provider: 'custom',
  text: string,
  tokens_in: number,
  tokens_out: number,
  duration_ms: number,
  tool_calls: any[],
  success: boolean,
  meta: {
    source: 'custom_api',
    response_data: any,
    tool_calls_detected: boolean
  }
}
```

## üîÑ **Retry & Error Handling**

### **Robustes Retry-System**

- Exponential Backoff (2^attempt \* 1000ms)
- Konfigurierbare Retry-Versuche
- Detaillierte Fehlerprotokollierung

### **Fallback-System**

- Automatische Fallback-Antworten bei Fehlern
- Timeout-Management (60s Standard)
- Connection Testing

## üîç **Health Check & Monitoring**

### **Verbindungstest**

- Endpoint-Erreichbarkeit
- HTTP Status Code Pr√ºfung
- Detaillierte Diagnose-Informationen

### **Konfigurationsvalidierung**

- URL-Format Validierung
- API-Key Pr√ºfung
- Required Field Validation

## üéØ **Modell-Erkennung**

### **Erkannte Custom-Modelle**

- `custom*` - Alle Custom-Modelle
- `generic*` - Generic APIs
- `external*` - Externe Dienste
- `api*` - API-basierte Dienste
- `rest*` - REST APIs

## üîß **Dynamische Konfiguration**

### **Runtime Updates**

- `updateConfig()` - Aktualisiert Konfiguration zur Laufzeit
- Flexible Header-Konfiguration
- Anpassbare Request-Parameter

### **Benutzerdefinierte Erweiterungen**

- JSON-basierte Header-Konfiguration
- Custom Parameter Injection
- Flexible Response-Mapping

Der Custom Provider dient als universelle Schnittstelle f√ºr beliebige REST-basierte KI-APIs und Backend-Dienste mit maximaler Flexibilit√§t und robustem Fehlerhandling f√ºr Enterprise-Integrationen.

Basierend auf der analysierten `elizaProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Eliza Provider - Hauptfunktionen**

### **Kernfunktionen**

- `ElizaProvider.respond(messages)` - Hauptfunktion f√ºr regelbasierte Antworten
- `ElizaEngine.apply(message, context)` - Regelbasierte Nachrichtenverarbeitung

### **Konfigurationsmanagement**

- `loadElizaConfig()` - L√§dt und validiert Eliza-Konfiguration
- `validateConfigPart()` - Validierung von Konfigurationsdateien

### **Session Management**

- `resetSession()` - Setzt Session zur√ºck
- `getSessionInfo()` - Gibt Session-Informationen zur√ºck
- `updateConfig()` - Aktualisiert Konfiguration zur Laufzeit

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export const elizaProvider = new ElizaProvider();
```

### **ElizaProvider Methoden**

- `respond(messages)` - Hauptantwort-Generator
- `getSessionInfo()` - Session-Informationen
- `getConfig()` - Aktuelle Konfiguration
- `updateConfig()` - Konfiguration aktualisieren
- `resetSession()` - Session zur√ºcksetzen
- `addCustomRule()` - Benutzerdefinierte Regel hinzuf√ºgen

## üõ†Ô∏è **ElizaEngine - Regelverarbeitung**

### **Kernmethoden**

- `apply(message, context)` - Wendet Regeln auf Nachricht an
- `reflect(input)` - Wendet Reflexionsregeln an
- `checkRuleContext()` - Pr√ºft Kontextanforderungen
- `generateResponse()` - Generiert Antwort basierend auf Regel
- `extractToolParameters()` - Extrahiert Tool-Parameter
- `handleWikipediaSearch()` - Spezialbehandlung f√ºr Wikipedia

### **Statistik-Methoden**

- `getStats()` - Regel- und Match-Statistiken
- `addRule()` - F√ºgt Regel zur Laufzeit hinzu

## üìã **Befehls-System (Command Handler)**

### **Verf√ºgbare Systembefehle**

- `?help` / `/help` / `hilfe` - Zeigt Hilfe an
- `?tools` - Zeigt verf√ºgbare Tools
- `?workflows` - Zeigt aktive Workflows
- `?config` - Zeigt Konfiguration & Regeln
- `?session` - Zeigt aktuelle Sitzung
- `?stats` - Zeigt Systemstatistiken
- `?rules` - Zeigt Regel-Statistiken
- `?status` - Zeigt Systemstatus

### **Befehls-Handler Methoden**

- `showHelp()` - Hilfe-Informationen
- `showTools()` - Tool-√úbersicht
- `showWorkflows()` - Workflow-Liste
- `showConfig()` - Konfigurationsdetails
- `showSession()` - Session-Info
- `showStats()` - Systemstatistiken
- `showRules()` - Regel-Statistiken
- `showStatus()` - Systemstatus

## ‚öôÔ∏è **Konfigurationssystem**

### **Konfigurationsquellen**

1. **Multi-File Directory** (`/data` Verzeichnis)
2. **Fallback File** (`context.json`)
3. **Default Configuration** (Integriert)

### **Konfigurationsstruktur**

```typescript
ElizaConfig {
  pools: Record<string, string[][]>;    // Antwort-Pools
  eliza_rules: ElizaRule[];            // Regeln
  reflections: Record<string, string>;  // Reflexionsregeln
  metadata: {...}                      // Metadaten
}
```

## üõ†Ô∏è **Tool Integration**

### **Tool Execution**

- `executeToolCalls(tool_calls)` - F√ºhrt Tool-Aufrufe aus
- `formatToolResults(results)` - Formatierung von Tool-Ergebnissen
- Integration mit `toolRegistry.call()`

### **Tool-Call Erkennung**

- Automatische Tool-Ausf√ºhrung basierend auf Regeln
- Parameter-Extraktion aus Regex-Matches
- Fehlerbehandlung f√ºr Tool-Fehler

## üìä **Response-System**

### **Antwort-Generierung**

- Regelbasierte Antworten
- Kontextabh√§ngige Antworten
- Tool-Ergebnis-Integration
- Fallback-Antworten

### **Response-Typen**

- **Regelbasierte Antworten** - Gefundene Pattern-Matches
- **Tool-Responses** - Mit Tool-Ergebnissen
- **Befehls-Responses** - Systembefehle
- **Fallback-Responses** - Bei keinem Match
- **Error-Responses** - Bei Fehlern

## üîÑ **Session Management**

### **Session-Informationen**

- Session-ID mit Zeitstempel
- Nachrichten-Historie
- Kontext-Zustand
- Laufzeit-Statistiken

### **Session-Methoden**

- Automatische Session-Erstellung
- Session-Reset mit neuer ID
- Historie-Begrenzung (25 Nachrichten Standard)

## üìà **Statistik & Monitoring**

### **Regel-Statistiken**

- Anzahl Regeln (gesamt/aktiv)
- Treffer nach Priorit√§t
- Letzter Treffer-Zeitpunkt
- Regeln mit Tools/Actions

### **System-Statistiken**

- Nachrichtenanzahl
- Aktives Thema
- Stimmungsanalyse
- Kontext-Confidence
- Durchschnittliche Response-Time

## üîß **Erweiterte Funktionalit√§t**

### **Dynamische Regeln**

- `addCustomRule()` - F√ºgt Regeln zur Laufzeit hinzu
- Regel-Validierung und Kompilierung
- Priorit√§ts-basierte Sortierung

### **Kontext-Integration**

- Integration mit `ConversationContext`
- Themenanalyse
- Stimmungserkennung
- Intent-Erkennung

## üéØ **Spezialbehandlungen**

### **Wikipedia Integration**

- Automatische Erkennung von Wikipedia-Suchanfragen
- Integration mit `wikipedia_search` Tool
- Fehlerbehandlung f√ºr Wikipedia-Fehler

### **Reflexionssystem**

- Automatische Text-Transformation
- Pronomen-Reflexion (ich ‚Üí du, etc.)
- Kontextuelle Anpassungen

Der Eliza Provider dient als regelbasierter Fallback-Provider mit erweiterter Tool-Integration, Session-Management und umfangreichem Diagnose-System f√ºr das ERP-KI-System.

Basierend auf der analysierten `fallbackProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Fallback Provider - Hauptfunktionen**

### **Kernfunktion**

- `callFallback(model, messages)` - Generiert Fallback-Antworten

### **Utility-Funktionen**

- `isFallbackModel(modelId)` - Pr√ºft auf Fallback-Modelle

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export const fallbackProvider = {
  call: callFallback, // Hauptaufruffunktion
  isModel: isFallbackModel, // Modellpr√ºfung
};
```

## üìã **Antwort-System**

### **Fallback-Antworten**

Vordefinierte Antwort-Pool:

```typescript
const FALLBACK_RESPONSES = [
  "Ich habe Ihre Eingabe registriert, ben√∂tige jedoch mehr Informationen.",
  "Die Anfrage konnte nicht eindeutig interpretiert werden.",
  "Bitte formulieren Sie die Frage etwas pr√§ziser.",
  "Im aktuellen Modus stehen nur einfache Antworten bereit.",
  "Gerne ‚Äì bitte geben Sie weitere Details an.",
];
```

### **Response-Generierung**

- Zuf√§llige Auswahl aus Antwort-Pool
- Einfache Text-Antwort ohne komplexe Verarbeitung

## üìä **Response-Struktur**

```typescript
AIResponse {
  text: string,              // Zuf√§llige Fallback-Antwort
  meta: {
    provider: "fallback",    // Provider-Identifikation
    model: string,           // √úbergebenes Modell oder "fallback"
    source: "local"          // Lokale Quelle
  }
}
```

## üéØ **Modell-Erkennung**

### **Erkannte Fallback-Modelle**

- `fallback` - Explizites Fallback-Modell
- `local` - Lokales Modell
- `offline*` - Offline-Modelle (enth√§lt "offline")

### **Fallback-Verhalten**

- Bei leerem `modelId` wird `true` zur√ºckgegeben
- Case-insensitive Pr√ºfung

## üîÑ **Integrations-Punkte**

### **Minimales Interface**

- Keine Tool-Integration
- Keine Kontext-Verarbeitung
- Keine Session-Verwaltung
- Keine komplexe Logik

### **Einsatzszenario**

- Letzte Fallback-Ebene bei Fehlern
- Offline-Betrieb
- Minimale Abh√§ngigkeiten
- Schnelle Antwort-Generierung

Der Fallback Provider dient als ultimative R√ºckfallebene mit minimaler Funktionalit√§t f√ºr Notf√§lle und Offline-Betrieb im ERP-KI-System.

Basierend auf der analysierten `huggingfaceProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **HuggingFace Provider - Hauptfunktionen**

### **Kernfunktion**

- `callHuggingFace(model, messages)` - Hauptfunktion f√ºr HuggingFace API Aufrufe

### **Utility-Funktionen**

- `buildHeaders()` - Erstellt Request-Header mit API-Key
- `formatInput(messages)` - Formatiert Chat-Nachrichten f√ºr API
- `isHuggingFaceModel(modelId)` - Pr√ºft auf HuggingFace-Modelle
- `testHuggingFace()` - Testet API-Erreichbarkeit

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export default {
  config: hfConfig, // Provider-Konfiguration
  call: callHuggingFace, // Hauptaufruffunktion
  isModel: isHuggingFaceModel, // Modellpr√ºfung
  test: testHuggingFace, // Verbindungstest
};
```

## ‚öôÔ∏è **Konfigurationssystem**

### **Umgebungsvariablen**

- `HF_MODEL` - Modellname (Standard: "mistralai/Mistral-7B-Instruct-v0.1")
- `HF_ENDPOINT` - API Endpoint (Standard: HuggingFace Inference API)
- `HUGGINGFACEHUB_API_TOKEN` - API Token (erforderlich)
- `HF_TEMPERATURE` - Temperatureinstellung
- `HF_MAX_TOKENS` - Maximale Tokens

### **Provider-Konfiguration**

```typescript
hfConfig: AIModuleConfig {
  name: "huggingfaceProvider",
  provider: "huggingface",
  model: string,                    // Verwendetes Modell
  endpoint: string,                 // API Endpoint
  api_key_env: string,              // API Key Umgebungsvariable
  temperature: number,              // 0.4 Standard
  max_tokens: number,               // 1200 Standard
  capabilities: string[],           // Unterst√ºtzte Funktionen
  active: boolean                   // Aktivierungsstatus
}
```

## üåê **API-Integration**

### **Request-Format**

```typescript
{
  inputs: string,                  // Formatierte Eingabe
  parameters: {
    temperature: number,
    max_new_tokens: number,
    return_full_text: boolean      // false - nur neue Tokens
  }
}
```

### **Response-Verarbeitung**

Unterst√ºtzt verschiedene Response-Formate:

- **Array-Format** - Standard Text-Generation
- **generated_text** - Direkte Text-Antwort
- **translation_text** - √úbersetzungs-Response
- **summary_text** - Zusammenfassungs-Response
- **outputs** - Generische Ausgaben

## üìä **Response-Struktur**

```typescript
AIResponse {
  text: string,                    // Verarbeitete Antwort
  data: any,                       // Rohdaten von API
  meta: {
    provider: "huggingface",
    model: string,
    tokens_used: number,           // Verwendete Tokens
    time_ms: number,               // Laufzeit
    source: string                 // Endpoint URL
  },
  errors?: string[]                // Bei Fehlern
}
```

## üõ†Ô∏è **Unterst√ºtzte Capabilities**

### **Modell-Typen**

- `chat` - Chat-Modelle
- `text` - Text-Generation
- `embedding` - Embedding-Modelle
- `translation` - √úbersetzungs-Modelle
- `summarization` - Zusammenfassungs-Modelle

### **Modell-Erkennung**

- **Slash-Notation** - Modelle mit `/` (z.B. "org/model")
- **HuggingFace Keywords** - Enth√§lt "huggingface"

## üîê **Sicherheit & Error Handling**

### **Authentifizierung**

- Bearer Token Authentication
- API-Key Validierung
- Header-basierte Authentifizierung

### **Fehlerbehandlung**

- HTTP Status Code √úberpr√ºfung
- Timeout Management (60s)
- Detaillierte Fehlerprotokollierung
- Fallback Error Responses

## üß™ **Health Check**

### **Verbindungstest**

- `testHuggingFace()` - Endpoint-Erreichbarkeit
- HEAD Request mit Timeout (5s)
- Boolean R√ºckgabe (true/false)

### **Endpoint-Validierung**

- Base URL Sicherung gegen undefined
- Pfad-Korrektur f√ºr Model-Endpoints
- URL-Format Validierung

## üîÑ **Input-Formatierung**

### **Nachrichten-Format**

```
role: content
role: content
```

Beispiel:

```
user: Hallo, wie geht es dir?
assistant: Mir geht es gut, danke!
```

### **Eingabe-Verarbeitung**

- Kombiniert alle Nachrichten zu einem String
- Beh√§lt Rollen-Informationen bei
- Einfache Text-Konkatenierung

Der HuggingFace Provider bietet eine robuste Integration der HuggingFace Inference API mit Unterst√ºtzung f√ºr verschiedene Modell-Typen und umfassender Fehlerbehandlung f√ºr das ERP-KI-System.

Basierend auf der analysierten `llamaCppProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Llama.cpp Provider - Hauptfunktionen**

### **Kernfunktion**

- `callLlamaCpp(model, messages)` - Hauptfunktion f√ºr llama.cpp API Aufrufe

### **Utility-Funktionen**

- `buildPrompt(messages)` - Baut Prompt aus Chat-Nachrichten
- `buildPayload(prompt)` - Erstellt Request-Payload
- `isLlamaCppModel(modelId)` - Pr√ºft auf llama.cpp-Modelle
- `testLlamaCpp()` - Testet API-Erreichbarkeit

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export default {
  config: llamaConfig, // Provider-Konfiguration
  call: callLlamaCpp, // Hauptaufruffunktion
  isModel: isLlamaCppModel, // Modellpr√ºfung
  test: testLlamaCpp, // Verbindungstest
};
```

## ‚öôÔ∏è **Konfigurationssystem**

### **Umgebungsvariablen**

- `LLAMA_CPP_MODEL` - Modellname (Standard: "local-gguf")
- `LLAMA_CPP_URL` - API Endpoint (Standard: "http://localhost:8080/completion")
- `LLAMA_CPP_TEMPERATURE` - Temperatureinstellung
- `LLAMA_CPP_MAX_TOKENS` - Maximale Tokens

### **Provider-Konfiguration**

```typescript
llamaConfig: AIModuleConfig {
  name: "llamaCppProvider",
  provider: "local",
  model: string,                    // Verwendetes Modell
  endpoint: string,                 // API Endpoint
  temperature: number,              // 0.4 Standard
  max_tokens: number,               // 1000 Standard
  capabilities: string[],           // Unterst√ºtzte Funktionen
  active: boolean                   // Aktivierungsstatus
}
```

## üåê **API-Integration**

### **Request-Format**

```typescript
{
  prompt: string,                  // Formatierter Prompt
  temperature: number,
  max_tokens: number,
  stream: boolean                  // false - kein Streaming
}
```

### **Prompt-Formatierung**

```
USER: Nachricht 1
ASSISTANT: Antwort 1
USER: Nachricht 2
ASSISTANT:
```

### **Response-Verarbeitung**

Unterst√ºtzt verschiedene Response-Formate:

- `content` - Standard llama.cpp Response
- `response` - Alternative Response-Felder
- `choices[0].text` - OpenAI-kompatibles Format
- `choices[0].message.content` - Chat Completion Format

## üìä **Response-Struktur**

```typescript
AIResponse {
  text: string,                    // Verarbeitete Antwort
  meta: {
    provider: "llama.cpp",
    model: string,
    time_ms: number,               // Laufzeit in Millisekunden
    source: string                 // Endpoint URL
  },
  errors?: string[],               // Bei Fehlern
  confidence?: number              // Konfidenz bei Fehlern (0)
}
```

## üõ†Ô∏è **Unterst√ºtzte Capabilities**

### **Modell-Typen**

- `chat` - Chat-Modelle
- `text` - Text-Generation
- `reasoning` - Reasoning-F√§higkeiten
- `tools` - Tool-Unterst√ºtzung
- `json` - JSON-Formatierung

### **Modell-Erkennung**

- **GGUF-Modelle** - Enth√§lt "gguf"
- **Llama-Modelle** - Enth√§lt "llama"
- **Lokale Modelle** - Enth√§lt "local"

## üîê **Sicherheit & Error Handling**

### **Authentifizierung**

- Keine Authentifizierung erforderlich (lokal)
- Einfache HTTP Requests
- Localhost-basierte Kommunikation

### **Fehlerbehandlung**

- HTTP Status Code √úberpr√ºfung
- L√§ngeres Timeout (120s f√ºr lokale Modelle)
- Detaillierte Fehlerprotokollierung
- Fallback Error Responses mit Confidence 0

## üß™ **Health Check**

### **Verbindungstest**

- `testLlamaCpp()` - Endpoint-Erreichbarkeit
- HEAD Request mit kurzem Timeout (3s)
- Boolean R√ºckgabe (true/false)

### **Endpoint-Validierung**

- Default URL Fallback
- URL-Format Sicherung

## üîÑ **Kompatibilit√§t**

### **API-Endpoints Unterst√ºtzt**

- `/completion` - Standard llama.cpp Endpoint
- `/chat/completions` - OpenAI-kompatible Endpoints

### **Response-Formate**

- Native llama.cpp Response
- OpenAI-kompatible Response-Struktur
- Einfache Text-Responses

Der Llama.cpp Provider bietet eine robuste Integration f√ºr lokale llama.cpp Instanzen mit breiter Kompatibilit√§t f√ºr verschiedene Modelle und API-Formate, ideal f√ºr Offline-Betrieb im ERP-KI-System.

Basierend auf der analysierten `localProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Local Provider - Hauptfunktionen**

### **Kernfunktionen**

- `callLocalModel(model, messages, options)` - Hauptfunktion f√ºr lokale Modellaufrufe
- `scanLocalModels()` - Scannt und erkennt lokale Modelle
- `updateLocalConfig(updates)` - Aktualisiert Konfiguration dynamisch
- `getLocalProviderStatus()` - Gibt Systemstatus zur√ºck

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export default {
  callLocalModel, // Hauptaufruffunktion
  scanLocalModels, // Modell-Erkennung
  updateLocalConfig, // Konfigurationsupdate
  getLocalProviderStatus, // Systemstatus
};
```

## üìÅ **Modell-Erkennungssystem**

### **Unterst√ºtzte Modell-Typen**

- **GGUF** - `.gguf` Dateien
- **HuggingFace** - `pytorch_model` Dateien
- **Whisper** - Whisper-Modelle
- **Falcon** - Falcon-Modelle
- **Mistral** - Mistral-Modelle
- **Gemma** - Gemma-Modelle
- **Qwen** - Qwen-Modelle
- **Unknown** - Andere Modelltypen

### **Scan-Pfade**

1. `F:/KI/models` - Externe KI-Modelle
2. `ERP_SteinmetZ_V1/models` - Projekt-interne Modelle

### **Modell-Informationen**

```typescript
LocalModelInfo {
  name: string,              // Modellname
  path: string,              // Dateipfad
  type: string,              // Modelltyp
  sizeMB?: number,           // Gr√∂√üe in MB
  lastModified?: string,     // √Ñnderungsdatum
  files?: string[]           // Dateiliste
}
```

## ‚öôÔ∏è **Konfigurationssystem**

### **Provider-Konfiguration**

```typescript
localProviderConfig: AIModuleConfig {
  name: "localProvider",
  provider: "local",
  model: "auto",                    // Automatische Modellerkennung
  role: "assistant",
  temperature: 0.4,
  max_tokens: 512,
  active: true,
  capabilities: string[]            // Unterst√ºtzte Funktionen
}
```

### **Dynamische Konfiguration**

- Runtime Updates via `updateLocalConfig()`
- Systemprompt-basierte Konfiguration
- Flexible Parameter-Anpassung

## üìä **Response-System**

### **Response-Struktur**

```typescript
AIResponse {
  text: string,                    // Antworttext
  action?: string,                 // Aktion (bei Fehlern)
  errors?: string[],               // Fehlermeldungen
  meta: {
    model: string,                 // Modellname
    source: "localProvider",       // Provider-Identifikation
    reasoning: string,             // Erkl√§rungs-Text
    confidence: number,            // Konfidenzwert (0.6)
    time_ms: number                // Laufzeit (75ms)
  }
}
```

### **Fehlerbehandlung**

- Modell-Nicht-Gefunden Fehler
- Verf√ºgbare Modelle Auflistung
- Detaillierte Fehlerinformationen

## üñ•Ô∏è **System-Status & Monitoring**

### **Status-Informationen**

```typescript
{
  provider: "localProvider",
  model_count: number,             // Anzahl gefundener Modelle
  directories: string[],           // Scan-Pfade
  active_config: AIModuleConfig,   // Aktuelle Konfiguration
  system_info: {
    hostname: string,              // System-Hostname
    platform: string,              // Betriebssystem
    arch: string,                  // Prozessor-Architektur
    totalmem_GB: number,           // Gesamter RAM
    freemem_GB: number,            // Freier RAM
    cpus: number,                  // CPU-Kerne
    uptime_hours: number           // System-Laufzeit
  }
}
```

## üõ†Ô∏è **Unterst√ºtzte Capabilities**

### **Funktionen**

- `tools` - Tool-Unterst√ºtzung
- `workflow` - Workflow-Integration
- `chat` - Chat-Funktionalit√§t
- `reasoning` - Reasoning-F√§higkeiten
- `json` - JSON-Verarbeitung

## üîÑ **Simulations-Modus**

### **Aktuelle Implementierung**

- **Mock/Simulation** - Keine echte Inferenz
- **Response-Generierung** - Vordefinierte Antworten
- **Erweiterbar** - Kann f√ºr echte Inferenz angepasst werden

### **Antwort-Format**

```
Systemprompt

üß† (Modellname): Ich habe deine Eingabe erhalten: "Eingabe" ...
Ich bin aktuell eine lokale Simulation, kann aber f√ºr echte Inferenz erweitert werden.
```

## üéØ **Einsatzszenarien**

### **Modell-Management**

- Automatische Modell-Erkennung
- Modell-Informationen Abfrage
- Pfad-basierte Organisation

### **System-Integration**

- Offline-Betrieb
- Lokale Modell-Nutzung
- Systemressourcen-√úberwachung

Der Local Provider dient als Modell-Management und Simulations-System f√ºr lokale KI-Modelle mit umfassender Systemintegration und Erweiterbarkeit f√ºr echte Inferenz im ERP-KI-System.

Basierend auf der analysierten `ollamaProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Ollama Provider - Hauptfunktionen**

### **Kernfunktionen**

- `callOllama(model, messages, options)` - Hauptfunktion f√ºr Ollama API Aufrufe
- `listOllamaModels()` - Listet verf√ºgbare Ollama-Modelle
- `updateOllamaConfig(update)` - Aktualisiert Konfiguration dynamisch
- `getOllamaStatus()` - Gibt Systemstatus zur√ºck

### **Tool Integration**

- `detectToolCalls(text)` - Erkennt Tool-Aufrufe im Text
- `handleToolCalls(toolCalls)` - F√ºhrt Tool-Aufrufe aus
- `safeParseJSON(str)` - Sicheres JSON-Parsing f√ºr Parameter

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export default {
  callOllama, // Hauptaufruffunktion
  listOllamaModels, // Modell-Liste
  updateOllamaConfig, // Konfigurationsupdate
  getOllamaStatus, // Systemstatus
};
```

## ‚öôÔ∏è **Konfigurationssystem**

### **Umgebungsvariablen**

- `OLLAMA_MODEL` - Standardmodell (Default: "mistral:latest")
- `OLLAMA_TEMPERATURE` - Temperatureinstellung
- `OLLAMA_MAX_TOKENS` - Maximale Tokens
- `OLLAMA_API_URL` - API Endpoint (Default: "http://localhost:11434")

### **Provider-Konfiguration**

```typescript
ollamaConfig: AIModuleConfig {
  name: "ollamaProvider",
  provider: "ollama",
  model: string,                    // Verwendetes Modell
  temperature: number,              // 0.5 Standard
  max_tokens: number,               // 1024 Standard
  active: boolean,                  // Aktivierungsstatus
  capabilities: string[],           // Unterst√ºtzte Funktionen
  timeout_ms: number                // 60000ms Timeout
}
```

## üåê **API-Integration**

### **Request-Format**

```typescript
{
  model: string,                    // Modellname
  stream: false,                    // Kein Streaming
  options: {
    temperature: number,
    num_predict: number             // Maximale Tokens
  },
  messages: Array<{                 // Nachrichten-Array
    role: string,
    content: string
  }>
}
```

### **Message-Struktur**

- Automatische System-Prompt Integration
- Beibehaltung der Chat-Historie
- Flexible Rollen-Zuweisung

## üõ†Ô∏è **Tool Integration System**

### **Tool-Call Erkennung**

**Syntax**: `[TOOL: tool_name {"param": "value"}]`

### **Tool-Execution Flow**

1. **Erkennung** - Regex-basierte Tool-Erkennung im Antworttext
2. **Parameter-Parsing** - Sicheres JSON-Parsing
3. **Ausf√ºhrung** - √úber `toolRegistry.call()`
4. **Ergebnis-Formatierung** - Erfolgs-/Fehler-Meldungen

### **Response-Integration**

- Tool-Ergebnisse werden an Antworttext angeh√§ngt
- Separate Erfolgs-/Fehler-Nachrichten
- Vollst√§ndige Transparenz √ºber Tool-Ausf√ºhrung

## üìä **Response-Struktur**

```typescript
AIResponse {
  text: string,                    // Kombinierte Antwort + Tool-Results
  action: "ollama_chat",           // Aktionstyp
  tool_calls: Array<{              // Erkannte Tool-Aufrufe
    name: string,
    parameters: any
  }>,
  meta: {
    model: string,                 // Verwendetes Modell
    tokens_used: number,           // Verwendete Tokens
    time_ms: number,               // Laufzeit
    source: "ollamaProvider",      // Provider-Identifikation
    confidence: number             // 0.95 Standard
  },
  errors?: string[]                // Bei Fehlern
}
```

## üîç **Modell-Management**

### **Modell-Liste**

```typescript
{
  name: string,                    // Modellname
  modified: string                 // √Ñnderungsdatum
}
```

### **API-Endpoints**

- `/api/tags` - Modell-Liste abrufen
- `/api/chat` - Chat-Completions

## üìà **Status & Monitoring**

### **System-Status**

```typescript
{
  provider: "ollama",
  apiUrl: string,                  // API Endpoint
  model_count: number,             // Anzahl verf√ºgbarer Modelle
  models: Array<{...}>,           // Modell-Liste
  config: AIModuleConfig,          // Aktuelle Konfiguration
  system: {
    hostname: string,              // System-Hostname
    platform: string,              // Betriebssystem
    arch: string,                  // Prozessor-Architektur
    totalmem_GB: number,           // Gesamter RAM
    freemem_GB: number,            // Freier RAM
    cpus: number,                  // CPU-Kerne
    uptime_h: number               // System-Laufzeit in Stunden
  }
}
```

## üõ°Ô∏è **Fehlerbehandlung**

### **Error Responses**

- HTTP Status Code Validierung
- Timeout Management (60s)
- Detaillierte Fehlerprotokollierung
- Confidence 0 bei Fehlern

### **Tool-Fehlerbehandlung**

- Separate Fehlerbehandlung pro Tool
- Fehlermeldungen in Response integriert
- Kein Abbruch bei Tool-Fehlern

## üéØ **Unterst√ºtzte Capabilities**

### **Funktionen**

- `chat` - Chat-Funktionalit√§t
- `embedding` - Embedding-Erstellung
- `vision` - Bildverarbeitung
- `tools` - Tool-Integration
- `workflow` - Workflow-Unterst√ºtzung
- `json` - JSON-Response-Formatierung

Der Ollama Provider bietet eine umfassende Integration f√ºr lokale Ollama-Instanzen mit erweiterter Tool-Unterst√ºtzung, Modell-Management und System-Monitoring f√ºr das ERP-KI-System.

Basierend auf der analysierten `openaiProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **OpenAI Provider - Hauptfunktionen**

### **Kernfunktionen**

- `callOpenAI(model, messages, options)` - Hauptfunktion f√ºr OpenAI API Aufrufe
- `updateOpenAIConfig(update)` - Aktualisiert Konfiguration dynamisch
- `getOpenAIStatus()` - Gibt Provider-Status zur√ºck

### **Tool Integration**

- `detectToolCalls(text)` - Erkennt Tool-Aufrufe im Text
- `handleToolCalls(calls)` - F√ºhrt Tool-Aufrufe aus
- `safeJsonParse(s)` - Sicheres JSON-Parsing f√ºr Parameter

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export default {
  callOpenAI, // Hauptaufruffunktion
  updateOpenAIConfig, // Konfigurationsupdate
  getOpenAIStatus, // Statusabfrage
};
```

## ‚öôÔ∏è **Konfigurationssystem**

### **Umgebungsvariablen**

- `OPENAI_API_KEY` - API-Schl√ºssel (erforderlich)
- `OPENAI_MODEL` - Standardmodell (Default: "gpt-4o-mini")
- `OPENAI_TEMPERATURE` - Temperatureinstellung
- `OPENAI_MAX_TOKENS` - Maximale Tokens

### **Provider-Konfiguration**

```typescript
openaiConfig: AIModuleConfig {
  name: "openaiProvider",
  provider: "openai",
  model: string,                    // Verwendetes Modell
  temperature: number,              // 0.3 Standard
  max_tokens: number,               // 1500 Standard
  active: boolean,                  // Aktivierungsstatus
  capabilities: string[],           // Unterst√ºtzte Funktionen
  timeout_ms: number,               // 60000ms Timeout
  description: string               // Beschreibung
}
```

## üåê **API-Integration**

### **Request-Format**

```typescript
{
  model: string,                    // Modellname
  messages: Array<{                 // Nachrichten-Array
    role: string,
    content: string
  }>,
  temperature: number,
  max_tokens: number,
  stream: false,                    // Kein Streaming
  response_format: string           // "text" Standard
}
```

### **Message-Struktur**

- Automatische System-Prompt Integration
- Beibehaltung der Chat-Historie
- Formatierung f√ºr OpenAI API

## üõ†Ô∏è **Tool Integration System**

### **Tool-Call Erkennung**

**Syntax**: `[TOOL: tool_name {"param": "value"}]`

### **Tool-Execution Flow**

1. **Erkennung** - Regex-basierte Tool-Erkennung im Antworttext
2. **Parameter-Parsing** - Sicheres JSON-Parsing
3. **Ausf√ºhrung** - √úber `toolRegistry.call()`
4. **Ergebnis-Formatierung** - Erfolgs-/Fehler-Meldungen

### **Response-Integration**

- Tool-Ergebnisse werden an Antworttext angeh√§ngt
- Separate Erfolgs-/Fehler-Nachrichten
- Vollst√§ndige Transparenz √ºber Tool-Ausf√ºhrung

## üìä **Response-Struktur**

```typescript
AIResponse {
  text: string,                    // Kombinierte Antwort + Tool-Results
  action: "openai_chat",           // Aktionstyp
  tool_calls: Array<{              // Erkannte Tool-Aufrufe
    name: string,
    parameters: any
  }>,
  meta: {
    model: string,                 // Verwendetes Modell
    tokens_used: number,           // Verwendete Tokens
    time_ms: number,               // Laufzeit
    source: "openaiProvider",      // Provider-Identifikation
    confidence: number             // 0.97 Standard (hoch)
  },
  errors?: string[]                // Bei Fehlern
}
```

## üîê **Client-Management**

### **API-Client Initialisierung**

- `getClient()` - Erstellt OpenAI Client mit API-Key
- API-Key Validierung
- Fehler bei fehlendem API-Key

## üìà **Status & Monitoring**

### **Status-Informationen**

```typescript
{
  provider: "openai",
  api_key_available: boolean,      // API-Key Verf√ºgbarkeit
  active_config: AIModuleConfig,   // Aktuelle Konfiguration
  default_model: string,           // Standardmodell
  capabilities: string[]           // Unterst√ºtzte Funktionen
}
```

## üõ°Ô∏è **Fehlerbehandlung**

### **Error Responses**

- API-Client Fehlerbehandlung
- Detaillierte Fehlerprotokollierung
- Confidence 0 bei Fehlern
- Strukturierte Error-Responses

### **Tool-Fehlerbehandlung**

- Separate Fehlerbehandlung pro Tool
- Fehlermeldungen in Response integriert
- Kein Abbruch bei Tool-Fehlern

## üéØ **Unterst√ºtzte Capabilities**

### **Funktionen**

- `chat` - Chat-Funktionalit√§t
- `tools` - Tool-Integration
- `reasoning` - Reasoning-F√§higkeiten
- `workflow` - Workflow-Unterst√ºtzung
- `json` - JSON-Response-Formatierung

## üîÑ **Dynamische Konfiguration**

### **Runtime Updates**

- `updateOpenAIConfig()` - Aktualisiert Konfiguration zur Laufzeit
- Flexible Parameter-Anpassung
- Sofortige Wirksamkeit

## üìã **Unterst√ºtzte Modelle**

### **OpenAI Modelle**

- `gpt-4o-mini` - Standardmodell
- `gpt-4o` - GPT-4 Omni
- `gpt-4-turbo` - GPT-4 Turbo
- `gpt-4` - GPT-4
- `gpt-3.5-turbo` - GPT-3.5 Turbo

Der OpenAI Provider bietet eine robuste Integration der OpenAI API mit erweiterter Tool-Unterst√ºtzung, dynamischer Konfiguration und umfassender Fehlerbehandlung f√ºr das ERP-KI-System.

Basierend auf der analysierten `vertexAIProvider.ts` Datei, hier sind die erkannten Funktionen und Routen:

## üß† **Vertex AI Provider - Hauptfunktionen**

### **Kernfunktionen**

- `callVertexAI(model, messages, options)` - Hauptfunktion f√ºr Vertex AI API Aufrufe
- `updateVertexConfig(update)` - Aktualisiert Konfiguration dynamisch
- `getVertexStatus()` - Gibt Provider-Status zur√ºck

### **Tool Integration**

- `detectToolCalls(text)` - Erkennt Tool-Aufrufe im Text
- `handleToolCalls(calls)` - F√ºhrt Tool-Aufrufe aus
- `safeJsonParse(s)` - Sicheres JSON-Parsing f√ºr Parameter

## üîß **Provider Interface**

### **Exportierte Hauptkomponente**

```typescript
export default {
  callVertexAI, // Hauptaufruffunktion
  updateVertexConfig, // Konfigurationsupdate
  getVertexStatus, // Statusabfrage
};
```

## ‚öôÔ∏è **Konfigurationssystem**

### **Umgebungsvariablen**

- `VERTEX_API_KEY` - API-Schl√ºssel (erforderlich)
- `VERTEX_MODEL` - Standardmodell (Default: "gemini-1.5-pro")
- `VERTEX_API_URL` - API Endpoint (Default: Google Generative Language API)
- `VERTEX_TEMPERATURE` - Temperatureinstellung
- `VERTEX_MAX_TOKENS` - Maximale Tokens

### **Provider-Konfiguration**

```typescript
vertexConfig: AIModuleConfig {
  name: "vertexAIProvider",
  provider: "vertex",
  model: string,                    // Verwendetes Modell
  endpoint: string,                 // API Endpoint
  api_key_env: string,              // API Key Umgebungsvariable
  temperature: number,              // 0.4 Standard
  max_tokens: number,               // 1500 Standard
  active: boolean,                  // Aktivierungsstatus
  capabilities: string[],           // Unterst√ºtzte Funktionen
  timeout_ms: number                // 60000ms Timeout
}
```

## üåê **API-Integration**

### **Request-Format**

```typescript
{
  contents: Array<{                // Nachrichten-Array
    role: string,
    parts: Array<{ text: string }>
  }>,
  generationConfig: {
    temperature: number,
    maxOutputTokens: number        // Maximale Ausgabe-Tokens
  }
}
```

### **Message-Struktur**

- **System-Prompt** - Wird automatisch vorangestellt
- **Parts-basierte Struktur** - Vertex AI spezifisches Format
- **Rollen-Trennung** - User/System/Assistant

### **API-Endpoint**

```
https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}
```

## üõ†Ô∏è **Tool Integration System**

### **Tool-Call Erkennung**

**Syntax**: `[TOOL: tool_name {"param": "value"}]`

### **Tool-Execution Flow**

1. **Erkennung** - Regex-basierte Tool-Erkennung im Antworttext
2. **Parameter-Parsing** - Sicheres JSON-Parsing
3. **Ausf√ºhrung** - √úber `toolRegistry.call()` (dynamischer Import)
4. **Ergebnis-Formatierung** - Erfolgs-/Fehler-Meldungen

### **Response-Integration**

- Tool-Ergebnisse werden an Antworttext angeh√§ngt
- Separate Erfolgs-/Fehler-Nachrichten
- Vollst√§ndige Transparenz √ºber Tool-Ausf√ºhrung

## üìä **Response-Struktur**

```typescript
AIResponse {
  text: string,                    // Kombinierte Antwort + Tool-Results
  action: "vertex_chat",           // Aktionstyp
  tool_calls: Array<{              // Erkannte Tool-Aufrufe
    name: string,
    parameters: any
  }>,
  meta: {
    provider: "vertexAI",          // Provider-Identifikation
    model: string,                 // Verwendetes Modell
    tokens_used: number,           // Verwendete Tokens
    time_ms: number,               // Laufzeit
    source: "vertexAIProvider",    // Quell-Identifikation
    confidence: number             // 0.95 Standard
  },
  errors?: string[]                // Bei Fehlern
}
```

## üîç **Response-Verarbeitung**

### **Sichere Response-Parsing**

- **Type-Safe Checking** - Robuste Objekt-Validierung
- **Multiple Response-Formate** - Unterst√ºtzt verschiedene Vertex AI Ausgabeformate:
  - `candidates[0].content.parts[0].text` - Standard-Textantwort
  - `candidates[0].output` - Alternative Ausgabe
- **Token-Count Extraction** - Aus `usageMetadata.totalTokenCount`

### **Fehlerbehandlung**

- API-Key Validierung
- HTTP Status Code √úberpr√ºfung
- Detaillierte Fehlerprotokollierung
- Confidence 0 bei Fehlern

## üìà **Status & Monitoring**

### **Status-Informationen**

```typescript
{
  provider: "vertexAI",
  model: string,                   // Aktuelles Modell
  endpoint: string,                // API Endpoint
  api_key_set: boolean,            // API-Key Verf√ºgbarkeit
  active_config: AIModuleConfig    // Aktuelle Konfiguration
}
```

## üéØ **Unterst√ºtzte Capabilities**

### **Funktionen**

- `chat` - Chat-Funktionalit√§t
- `vision` - Bildverarbeitung (Gemini Vision)
- `tools` - Tool-Integration
- `json` - JSON-Response-Formatierung
- `reasoning` - Reasoning-F√§higkeiten

## üîÑ **Dynamische Konfiguration**

### **Runtime Updates**

- `updateVertexConfig()` - Aktualisiert Konfiguration zur Laufzeit
- Flexible Parameter-Anpassung
- Sofortige Wirksamkeit

## üìã **Unterst√ºtzte Modelle**

### **Gemini Modelle**

- `gemini-1.5-pro` - Standardmodell
- `gemini-1.5-flash` - Schnelles Modell
- `gemini-pro` - Gemini Pro
- `gemini-ultra` - Gemini Ultra (falls verf√ºgbar)

Der Vertex AI Provider bietet eine robuste Integration der Google Vertex AI API mit Fokus auf Gemini-Modelle, erweiterter Tool-Unterst√ºtzung und umfassender Fehlerbehandlung f√ºr das ERP-KI-System.
